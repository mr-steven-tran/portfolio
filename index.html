<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Steven's Portfolio!</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.15.4/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Steven Tran</span>
                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profile.png" alt="..." /></span>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#portfolio">Portfolio</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#skills">Skills</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#education">Education</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#experience">Experience</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#interests">Interests</a></li>
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0">
                        Steven
                        <span class="text-primary">Tran</span>
                    </h1>
                    <div class="subheading mb-5">
                        Greater Phoenix Area · 
                        <a href="mailto:name@email.com">mistersteventran@gmail.com</a>
                    </div>
                    <p class="lead mb-5">I’m a skeptical, inquisitive data scientist with a background in auditing, taxation, and economic development in government. I use my experience in process improvement and research combined with skills in Python, SQL, statistical analysis, and machine learning to find creative solutions to complex problems.</p>
                    <div class="social-icons">
                        <a class="social-icon" href="https://www.linkedin.com/in/steven-tran/"><i class="fab fa-linkedin-in"></i></a>
                        <a class="social-icon" href="https://github.com/mr-steven-tran"><i class="fab fa-github"></i></a>
                        <a class="social-icon" href="https://steven-tran.medium.com/"><i class="fab fa-medium"></i></a>
                        <a class="social-icon" href="https://www.kaggle.com/stvntrn"><i class="fab fa-kaggle"></i></a>
                        <a class="social-icon" href="https://twitter.com/notsteventran"><i class="fab fa-twitter"></i></a>
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Portfolio-->
            <section class="resume-section" id="portfolio">
                <div class="resume-section-content">
                    <h2 class="mb-5">Portfolio</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Identifying Nutrient Deficiencies in Satellite Farmland Imagery</h3>
                            <div class="subheading mb-3"><a href="https://github.com/mr-steven-tran/identifying_nutrient_deficiencies_in_satellite_farmland_imagery">github.com/mr-steven-tran/identifying_nutrient_deficiencies_in_satellite_farmland_imagery</a></div>
                            <p>
                                For this project, I trained a <a href='https://arxiv.org/abs/1505.04597'>U-Net</a> convolutional neural network using Tensorflow Keras to perform semantic segmentation on satellite farmland imagery.
                                The goal was to identify image regions with  nutrient deficiency, prototyping a method by which a model could identify crop ailments that are detrimental to aricultural yields.
                            </p>
                            <p>
                                The image data came from the <a href="https://arxiv.org/abs/2001.01306">AgricultureVision dataset</a>, which is publicly available on the <a href="https://registry.opendata.aws/intelinair_agriculture_vision/">Registry of Open Data on AWS</a>.

                            </p>
                            <img src="assets/img/ag_preds.png" alt="Comparison of segmentation model predictions">
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Ames, Iowa Housing Price Prediction</h3>
                            <div class="subheading mb-3"><a href="https://github.com/mr-steven-tran/ames_IA_housingPrice_prediction">github.com/mr-steven-tran/ames_IA_housingPrice_prediction</a></div>
                            <p>
                                For this project, I used python, NumPy, pandas, sci-kit learn, and multivariate linear regression to solve two problems. The first was a prediction problem: knowing only the attributes of a set of homes in Ames, Iowa, predict what their sale prices would be. The second was an inferential problem: determine if certain home floor plan configurations fetch a higher sale price premium than others.
                            </p> 
                            <img src="assets/img/proj2_EDA_example.png" alt="Distribution of home sale price and number of bedrooms">
                            <P>
                                If you're interested, I wrote a <a href="https://steven-tran.medium.com/project-retrospective-the-ames-ia-home-sale-price-predictions-21fa03ae3870">brief retrospective on the project on Medium</a>.
                            </P>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Reddit Post Subreddit Classification</h3>
                            <div class="subheading mb-3"><a href="https://github.com/mr-steven-tran/subreddit_classification">github.com/mr-steven-tran/subreddit_classification</a></div>
                            <p>
                                In this project, I utilized Reddit's Pushshift API to collect over 15,000 user posts from two subreddits: /r/UnpopularOpinion and /r/Rant. Then, I used python, Numpy, sci-kit learn, and Natural Language Processing techniques like term-frequency-inverse-document-frequency vectorization to attempt to classify each post as belonging to one subreddit or the other.
                            </p>
                            <p>
                                For modeling, I fit, tuned, and compared performance for <strong>Logistic Regression</strong>, <strong>K-Nearest Neighbors</strong>, and <strong>XGBoost</strong> classification models.
                            </p>
                            <img src="assets/img/proj3_results.png" alt="Model performance comparison for Logistic Regression, K-Nearest Neighbors, and XGBoost">
                            <p>
                                <!--<a href="#">For this project, I also recreated the exploratory data analysis and modeling in <strong>R</strong>!</a>-->
                            </p>                            
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Predicting U.S. Crime</h3>
                            <div class="subheading mb-3"><a href="https://github.com/mr-steven-tran/predicting_US_crime">github.com/mr-steven-tran/predicting_US_crime</a></div>
                            <p>
                                For this project, I wrote call scripts to the Federal Bureau of Investigation Crime Data and the Bureau of Labor Statistics Public Data APIs to retrieve summary crime statistics by state over time as well as CPI and unemployment information over time. Working with two collaborators, I defined a forecasting problem using our target and predictor data: predict crime rates in 2021 given a time series of prior crime rate data.
                            </p>
                            <p>
                                EDA of the time series revealed that none of the states' crime rates over time exhibited stationarity, so we endeavored to use two modeling techniques:
                            </p>
                            <ul>
                                <li><strong>ARIMA</strong> (Auto-regressive Integrated Moving Average): A time series modeling technique which relies on the assumption that the endogenous series exhibits stationarity. Using second-order differencing, we were able to find stationarity in over 90% of the states' time series.</li>
                                <li><strong>Recurrent Neural Network with Long Short-Term Memory</strong>: A deep learning technique suitable for sequential modeling (i.e. time series) which does not require stationarity as an assumption.</li>
                            </ul>
                            <img src="assets/img/proj5_example.png" alt="Example RNN-LSTM 2021 property crime prediction for New Mexico.">
                        </div>
                    </div>                                        
                </div>
            </section>
            <hr class="m-0" />
            <!-- Skills-->
            <section class="resume-section" id="skills">
                <div class="resume-section-content">
                    <h2 class="mb-5">Skills</h2>
                    <div class="subheading mb-3">Programming Languages & Tools</div>
                    <ul>
                        <li>Python</li>
                        <!--<li>R</li>-->
                        <li>SQL</li>
                        <li>NumPy</li>
                        <li>Pandas</li>
                        <li>Sci-kit Learn</li>
                        <li>PySpark</li>
                        <li>Tensorflow + Keras</li>
                        <li>VBA for Microsoft Excel & Access</li>
                        <li>Django</li>
                    </ul>
                    <div class="subheading mb-3">Data Science Techniques</div>
                    <ul>
                        <li>Data Cleaning</li>
                        <li>Data Collection & Webscraping</li>
                        <li>Exploratory Data Analysis & Data Visualization</li>
                        <li>Model Tuning & Evaluation</li>
                    </ul>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Education-->
            <section class="resume-section" id="education">
                <div class="resume-section-content">
                    <h2 class="mb-5">Education</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Arizona State University | Tempe, AZ</h3>
                            <div class="subheading mb-3">Bachelor of Science, Economics</div>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2012 — May 2016</span></div>
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Experience-->
            <section class="resume-section" id="experience">
                <div class="resume-section-content">
                    <h2 class="mb-5">Experience</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0"><strong>Senior Analytic Consultant</strong></h3>
                            <div class="subheading mb-3"><a href="https://www.voyatek.com/">Voyatek</a></div>
                            <p>
                                <ul>
                                    <li>Used SQL, Python, Jupyter Notebooks, and Excel to engineer data pipeliness, create analytic datasets, and implement and evaluate machine learning models.</li>  
                                    <li>Led clients through the <a href="https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining">Cross-Industy Standard Process for Data Mining (CRISP-DM)</a> lifecycle.</li>
                                    <li>Worked cross-functionally with other teams to validate and support analytic product delivery.</li>
                                    <li>Trained client and internal modelers to use adopted tools and methodologies.</li>
                                    <li>Wrote documentation informing processes and procedures.</li>
                                    <li>Automated repetitive tasks in Python.</li>
                                </ul>    
                            </p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">January 2022 — Present</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Economist</h3>
                            <div class="subheading mb-3">Arizona Department of Revenue</div>
                            <p>
                                <ul>
                                    <li>Leveraged Excel, Access, and VBA to automate data entry and validation shortening process times by up to 80 percent.</li>
                                    <li>Developed JavaScript and VBA scripts to scrape AZ Legislature data on Legislative Committee Agendas.</li>
                                    <li>Performed analysis forecasting the revenue impact of pending legislative changes.</li>
                                    <li>Used T-SQL and Stored Procedures to develop reports and dashboards to monitor customer impact of process changes.</li>
                                    <li>Administered first-come first-serve tax credit approval program overseeing the transfer of over $300m in tax credit donations.</li>
                                </ul>
                            </p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">March 2018 — July 2021</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Auditor</h3>
                            <div class="subheading mb-3">The Office of the Auditor General, State of Arizona</div>
                            <p>
                                <ul>
                                    <li>Employed Excel and Access in analyzing public school district financial and performance data.</li>
                                    <li>Conducted inspections and interviews in several audit engagements resulting in delivery of audit findings.</li>
                                    <li>Synthesized observations, evidence, and analysis into a published chapter of the JTED Special Study ordered by the Arizona Legislature.</li>
                                </ul>
                            </p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">May 2016 — March 2018</span></div>
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Interests-->
            <section class="resume-section" id="interests">
                <div class="resume-section-content">
                    <h2 class="mb-5">Interests</h2>
                    <p>Outside of practicing data science, I'm an avid fan of road trips, hiking, and cooking. I also enjoy making home movies when schedule permits.</p>
                    <p class="mb-0">My preferred method of regular exercise is indoor rowing 🚣 (a great low-impact cardio option for the Arizona summers 🌞). A personal goal of mine is to row a half-marathon (21.1 km) without stopping.</p>
                </div>
            </section>
            <hr class="m-0" />
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
